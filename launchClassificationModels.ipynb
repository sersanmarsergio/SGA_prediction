{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import sklearn.neighbors._base\n",
    "import sys\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "\n",
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Set path to search waveform tables\n",
    "os.chdir(\"../../Data/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to one-hot encode the Clinical DF, and define the new variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_DF(df):\n",
    "\n",
    "    one_hot = OneHotEncoder()\n",
    "    # Woman Education\n",
    "    encoded = one_hot.fit_transform(df[['womanEducation']])\n",
    "    df[['no/primary', 'secondary/technology', 'university']] = encoded.toarray()\n",
    "    # Woman Work\n",
    "    encoded = one_hot.fit_transform(df[['womanWork']])\n",
    "    df[['unemployed', 'autonomous', 'private/student/other']] = encoded.toarray()\n",
    "    # Habits\n",
    "    encoded = one_hot.fit_transform(df[['habits']])\n",
    "    df[['habits_no', 'habits_stop_pregnancy', 'habits_yes']] = encoded.toarray()\n",
    "    # Dropping Unnecessary Columns\n",
    "    df = df.drop(columns=['womanEducation', 'womanWork', 'habits'])\n",
    "    # Re-arrange columsn to place outcomes at the end\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[:18] + cols[24:] + cols[18:24]\n",
    "    df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMPACT – Clinical Data + Fetal biometrics + EchoPI\n",
    "\n",
    "SGA encoded using the BW percentile calculated using the local standard (Figueras et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLINICAL DATA (Demographics + Previous/Current pregnancy + Mother Measurements)\n",
    "dfClinical = pd.read_csv('jointDF28WeeksIMPACT_SGA_Intergrowth-21.csv', index_col='ID')\n",
    "dfClinical.drop('dataset', axis=1, inplace=True)\n",
    "\n",
    "# Replacing NaNs in binary variables with 0 – Assumption of normality\n",
    "dfClinical[['highBloodPressure', 'convulsions', 'GDM', 'anaemiaOrIron', 'bleeding']] = dfClinical[[\n",
    "    'highBloodPressure', 'convulsions', 'GDM', 'anaemiaOrIron', 'bleeding']].fillna(0)\n",
    "\n",
    "# Drop samples missing continuous variables & outcomes\n",
    "dfClinical = dfClinical[dfClinical.isna().any(axis=1) == False]\n",
    "\n",
    "# One hot encoding categorical variables\n",
    "dfClinical = one_hot_encoding_DF(dfClinical)\n",
    "dfClinical.index = [int(idx[6:]) for idx in dfClinical.index]\n",
    "\n",
    "# FETAL BIOMETRICS\n",
    "dfBiometrics = pd.read_excel('IMPACT/IMPACT_ecocardio_raw.xlsx', index_col='Cod')\n",
    "dfBiometrics = dfBiometrics[['PC_ecoIMPACT','DBP_ecoIMPACT','LF_ecoIMPACT','PA_ecoIMPACT','EG_ecoIMPACT','EFW_Hadlock','EGparto_cod']]\n",
    "dfBiometrics.columns = ['HC', 'BPD', 'FL', 'AC', 'GA', 'EFW','GA_birth']\n",
    "dfBiometrics = dfBiometrics[dfBiometrics.isna().any(axis=1) == False]\n",
    "\n",
    "# ECHO PI\n",
    "dfEchoPI = pd.read_excel('IMPACT/IMPACT_ecocardio_raw.xlsx', index_col='Cod')\n",
    "dfEchoPI = dfEchoPI[['UA_ecoIMPACT','ACM_ecoIMPACT','CPR_calc']]\n",
    "dfEchoPI.columns = ['UA PI', 'MCA PI', 'CPR']\n",
    "dfEchoPI = dfEchoPI[dfEchoPI.isna().any(axis=1) == False]\n",
    "\n",
    "# KEEP ONLY COMMON INDICES\n",
    "commonIndices = [idx for idx in dfClinical.index if idx in dfBiometrics.index and idx in dfEchoPI.index]\n",
    "dfBiometrics = dfBiometrics.loc[commonIndices].round(decimals=2)\n",
    "dfEchoPI = dfEchoPI.loc[commonIndices].round(decimals=2)\n",
    "dfClinical = dfClinical.loc[commonIndices].round(decimals=2)\n",
    "\n",
    "dfImpact = pd.concat([dfClinical, dfBiometrics,dfEchoPI ], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FEDOC – Clinical Data + Fetal biometrics + EchoPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLINICAL DATA (Demographics + Previous/Current pregnancy + Mother Measurements)\n",
    "dfClinical = pd.read_csv('jointDF28WeeksFEDOC_SGA_Intergrowth-21.csv', index_col='ID')\n",
    "dfClinical.drop('dataset', axis=1, inplace=True)\n",
    "# Replacing NaNs in binary variables with 0 – Assumption of normality\n",
    "dfClinical[['highBloodPressure', 'convulsions', 'GDM', 'anaemiaOrIron', 'bleeding']] = dfClinical[[\n",
    "    'highBloodPressure', 'convulsions', 'GDM', 'anaemiaOrIron', 'bleeding']].fillna(0)\n",
    "\n",
    "\n",
    "dfGA_birth = pd.read_excel('FEDOC/Pilot Fedoc - full data file March 2022.xlsx', index_col='ML Study ID')\n",
    "dfGA_birth = dfGA_birth[['Gestational age in weeks']]\n",
    "dfGA_birth.columns = ['GA_birth']\n",
    "\n",
    "# Replacing NaNs in percentile with 0 – Not to drop death cases\n",
    "#dfClinical['Weight_percentile'] = dfClinical['Weight_percentile'].fillna(0)\n",
    "\n",
    "# Drop samples missing continuous variables & outcomes\n",
    "#dfClinical = dfClinical[dfClinical.isna().any(axis=1) == False]\n",
    "\n",
    "# One hot encoding\n",
    "dfClinical = one_hot_encoding_DF(dfClinical)\n",
    "dfClinical.index = [int(idx[5:]) for idx in dfClinical.index]\n",
    "\n",
    "# FETAL BIOMETRICS\n",
    "dfBiometrics = pd.read_csv('FEDOC/Form 3 - with biometry z score.csv', index_col='f3_ml_sid')\n",
    "# If studies are duplicated, keep only the FU visit (higher GA)\n",
    "idx = dfBiometrics.index\n",
    "to_keep = np.logical_not(idx.duplicated(keep='last'))\n",
    "dfBiometrics = dfBiometrics.loc[to_keep]\n",
    "dfBiometrics = dfBiometrics[['HC', 'BPD', 'FL', 'AC','GA_weeks','EFW_Hadlock']]\n",
    "dfBiometrics.columns = ['HC', 'BPD', 'FL', 'AC', 'GA', 'EFW']\n",
    "dfBiometrics = dfBiometrics[dfBiometrics.isna().any(axis=1) == False]\n",
    "\n",
    "# Echo PI\n",
    "dfEchoPI = pd.read_excel('FEDOC/EchoPIs - z score.xlsx', index_col='Study ID')\n",
    "# If studies are duplicated, keep only the FU visit (higher GA)\n",
    "dfEchoPI.index = [str(idx)[0:4] for idx in dfEchoPI.index]\n",
    "to_keep = np.logical_not(dfEchoPI.index.duplicated(keep='last'))\n",
    "dfEchoPI = dfEchoPI.loc[to_keep]\n",
    "dfEchoPI.index = [int(idx) for idx in dfEchoPI.index]\n",
    "dfEchoPI = dfEchoPI[['UA PI', 'MCA PI', 'CPR']]\n",
    "dfEchoPI = dfEchoPI[dfEchoPI.isna().any(axis=1) == False]\n",
    "\n",
    "# KEEP ONLY COMMON INDICES\n",
    "commonIndices = [idx for idx in dfClinical.index if idx in dfBiometrics.index and idx in dfEchoPI.index]\n",
    "\n",
    "dfBiometrics = dfBiometrics.loc[commonIndices].round(decimals=2)\n",
    "dfEchoPI = dfEchoPI.loc[commonIndices].round(decimals=2)\n",
    "dfClinical = dfClinical.loc[commonIndices].round(decimals=2)\n",
    "dfClinical = dfClinical.loc[commonIndices].round(decimals=2)\n",
    "dfGA_birth = dfGA_birth.loc[commonIndices].round(decimals=2)\n",
    "\n",
    "dfFedoc = pd.concat([dfClinical, dfBiometrics, dfEchoPI, dfGA_birth], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)\n",
    "dfFedoc[dfFedoc['neonatalDeath']==1]\n",
    "dfFedoc.isna().sum()\n",
    "dfFedoc.shape\n",
    "dfFedoc.describe()\n",
    "dfFedoc[dfFedoc['preterm'].isnull()].index.tolist() # Only 2 cases missing partum data – these are excluded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input variables to be used in the different (incremental) experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = {'Set1': ['womanAge', 'previouslyPregnant', 'previousPreterm', 'previousFetalDeath', 'antenatalCare', 'highBloodPressure', 'convulsions', 'GDM', 'anaemiaOrIron', 'feverOrAntibiotics', 'bleeding',\n",
    "                  'maternalWeight', 'maternalHeight', 'systolicBP', 'diastolicBP', 'hemoglobin', 'GA_meas', 'GA_US', 'no/primary', 'secondary/technology', 'university', 'unemployed', 'autonomous', 'private/student/other',\n",
    "                  'habits_no', 'habits_stop_pregnancy', 'habits_yes'], \n",
    "                  \n",
    "                  'Set2': ['HC', 'BPD', 'FL', 'AC', 'GA'],\n",
    "\n",
    "                  'Set3': ['UA PI', 'MCA PI', 'CPR', 'GA'], \n",
    "\n",
    "                  'Set4': ['womanAge', 'previouslyPregnant', 'previousPreterm', 'previousFetalDeath', 'antenatalCare', 'highBloodPressure', 'convulsions', 'GDM', 'anaemiaOrIron', 'feverOrAntibiotics', 'bleeding',\n",
    "                  'maternalWeight', 'maternalHeight', 'systolicBP', 'diastolicBP', 'hemoglobin', 'GA_meas', 'GA_US', 'no/primary', 'secondary/technology', 'university', 'unemployed', 'autonomous', 'private/student/other',\n",
    "                  'habits_no', 'habits_stop_pregnancy', 'habits_yes', 'HC', 'BPD', 'FL', 'AC'], \n",
    "\n",
    "                  'Set5': ['womanAge', 'previouslyPregnant', 'previousPreterm', 'previousFetalDeath', 'antenatalCare', 'highBloodPressure', 'convulsions', 'GDM', 'anaemiaOrIron', 'feverOrAntibiotics', 'bleeding',\n",
    "                  'maternalWeight', 'maternalHeight', 'systolicBP', 'diastolicBP', 'hemoglobin', 'GA_meas', 'GA_US', 'no/primary', 'secondary/technology', 'university', 'unemployed', 'autonomous', 'private/student/other',\n",
    "                  'habits_no', 'habits_stop_pregnancy', 'habits_yes', 'UA PI', 'MCA PI', 'CPR'],\n",
    "\n",
    "                  'Set6': ['HC', 'BPD', 'FL', 'AC', 'GA', 'UA PI', 'MCA PI', 'CPR'], \n",
    "\n",
    "                  'Set7': ['womanAge', 'previouslyPregnant', 'previousPreterm', 'previousFetalDeath', 'antenatalCare', 'highBloodPressure', 'convulsions', 'GDM', 'anaemiaOrIron', 'feverOrAntibiotics', 'bleeding',\n",
    "                  'maternalWeight', 'maternalHeight', 'systolicBP', 'diastolicBP', 'hemoglobin', 'GA_meas', 'GA_US', 'no/primary', 'secondary/technology', 'university', 'unemployed', 'autonomous', 'private/student/other',\n",
    "                  'habits_no', 'habits_stop_pregnancy', 'habits_yes', 'HC', 'BPD', 'FL', 'AC', 'UA PI', 'MCA PI', 'CPR']\n",
    "                  }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable inspection – Cohorts comparison (normalized histograms = probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for feat in input_features['Set1']:\n",
    "\n",
    "    tmp = dfImpact[feat]\n",
    "    bin_width = (max(tmp) - min(tmp)) / 10\n",
    "    if bin_width == 0: bin_width = .1\n",
    "    weights = np.ones_like(tmp)/float(len(tmp))\n",
    "    plt.hist(tmp, weights = weights, alpha=0.5, label='IMPACT', bins = np.arange(min(tmp), max(tmp) + bin_width, bin_width))\n",
    "    \n",
    "    tmp = dfFedoc[feat]\n",
    "    weights = np.ones_like(tmp)/float(len(tmp))\n",
    "    plt.hist(tmp, weights = weights, alpha=0.5, label='FEDOC', bins = np.arange(min(tmp), max(tmp) + bin_width, bin_width))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(feat)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable inspection – Class distribution (normalized histograms = probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Outcome = 'SGA'\n",
    "\n",
    "for feat in input_features['Set4']:\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(feat)\n",
    "\n",
    "    tmp = dfImpact.loc[dfImpact[Outcome]==0,feat]\n",
    "    bin_width = (max(tmp) - min(tmp)) / 10\n",
    "    if bin_width == 0: bin_width = .1\n",
    "    weights = np.ones_like(tmp)/float(len(tmp))\n",
    "    ax1.hist(tmp, weights = weights, bins = np.arange(min(tmp), max(tmp) + bin_width, bin_width),  alpha=0.5, label='Normal')\n",
    "    tmp = dfImpact.loc[dfImpact[Outcome]==1,feat]\n",
    "    weights = np.ones_like(tmp)/float(len(tmp))\n",
    "    ax1.hist(tmp, weights = weights, bins = np.arange(min(tmp), max(tmp) + bin_width, bin_width), alpha=0.5, label=Outcome)\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.set_title('IMPACT')\n",
    "    ax1.set_xlabel('Value')\n",
    "    ax1.set_ylabel('Probability')\n",
    "\n",
    "    tmp = dfFedoc.loc[dfFedoc[Outcome]==0,feat]\n",
    "    bin_width = (max(tmp) - min(tmp)) / 10\n",
    "    if bin_width == 0: bin_width = .1\n",
    "    weights = np.ones_like(tmp)/float(len(tmp))\n",
    "    ax2.hist(tmp, weights = weights, bins = np.arange(min(tmp), max(tmp) + bin_width, bin_width), alpha=0.5, label='Normal')\n",
    "    tmp = dfFedoc.loc[dfFedoc[Outcome]==1,feat]\n",
    "    weights = np.ones_like(tmp)/float(len(tmp))\n",
    "    ax2.hist(tmp, weights = weights, bins = np.arange(min(tmp), max(tmp) + bin_width, bin_width), alpha=0.5, label=Outcome)\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_title('FEDOC')\n",
    "    ax2.set_xlabel('Value')\n",
    "    ax2.set_ylabel('Probability')\n",
    "\n",
    "    plt.show()'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome variable to be used – remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Outcomes --> ['SGA', 'weightPercentile', 'cSection', 'neonatalDeath', 'birthWeight', 'preterm']\n",
    "\n",
    "Outcome = 'SGA'\n",
    "\n",
    "#Outcome = 'preterm < 36'\n",
    "#dfImpact[Outcome] = np.array(dfImpact['GA_birth'] < 36).astype(int)\n",
    "#dfFedoc[Outcome] = np.array(dfFedoc['GA_birth'] < 36).astype(int)\n",
    "\n",
    "#dfImpact['Low_BW'] = (dfImpact['birthWeight'] <= 2.5) * 1\n",
    "#dfFedoc['Low_BW'] = (dfFedoc['birthWeight'] <= 2.5) * 1\n",
    "#Outcome = 'Low_BW'\n",
    "\n",
    "dfImpact = dfImpact.dropna(subset=[Outcome])\n",
    "dfFedoc = dfFedoc.dropna(subset=[Outcome])\n",
    "\n",
    "print( (sum(dfImpact[Outcome])/len(dfImpact))*100)\n",
    "print( (sum(dfFedoc[Outcome])/len(dfFedoc))*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expDict = {'Exp1': ['X1_train','y1_train','X1_test','y1_test'], #Impact–Impact \n",
    "           'Exp2': ['X2_train','y2_train','X2_test','y2_test'], #Fedoc–Fedoc \n",
    "           'Exp3': ['X1_train','y1_train','X2_test','y2_test'], #Impact–Fedoc\n",
    "           'Exp4': ['X2_train','y2_train','X1_test','y1_test'], #Fedoc–Impact\n",
    "           }\n",
    "\n",
    "#expDict = {'Exp2': ['X2_train','y2_train','X2_test','y2_test']} #Fedoc–Fedoc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1-A Grid Search and Cross Validation\n",
    "\n",
    "Rather than the naive approach followed before, we now implement a cross-validation technique to find the model hyperparameters ('max_depth', 'n_estimators', 'learning_rate') that maximize the classification score (AUC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch_CV(X_train, y_train):\n",
    "    \n",
    "    w = (1 - np.mean(y_train))/np.mean(y_train)\n",
    "    estimator = XGBClassifier(scale_pos_weight = w, eval_metric = 'logloss', verbosity = 0, silent=True, nthread=4, seed=42)\n",
    "\n",
    "    parameters = {\n",
    "        'max_depth': range (1, 10, 1),\n",
    "        'n_estimators': range(50, 1001, 50),\n",
    "        'learning_rate': np.arange(0.1, 0.51, 0.1)\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=parameters,\n",
    "        scoring = 'roc_auc',\n",
    "        n_jobs = -1, # uses all available processors\n",
    "        cv = 5, # make (stratified) k-fold CV; using the StratifiedKFold method\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1-B Bayesian (hyperparameter) optimization and Cross Validation\n",
    "\n",
    "Rather than the brute force (grid search), which optimizes the true objective function, we now implement a Bayesian search, which optimizes a proxy function (surrogate function). Thus, we do exploration rather than (brute force) exploitation. With the drawback of finding a local (rather than a global) minima for the obtective function, this method favours computational efficiency, and allows us to search over a much larger space of hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BayesSearch_CV(X_train, y_train):\n",
    "    \n",
    "    w = (1 - np.mean(y_train))/np.mean(y_train)\n",
    "    estimator = XGBClassifier(scale_pos_weight = w, eval_metric = 'logloss', verbosity = 0, silent=True, seed=42)\n",
    "\n",
    "    # Setting the search space\n",
    "    search_spaces = {'learning_rate': Real(.1, .50, 'uniform'),\n",
    "                    'max_depth': Integer(2, 10),\n",
    "                    'subsample': Real(0.1, 1.0, 'uniform'),\n",
    "                    'colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns by tree\n",
    "                    'reg_lambda': Real(1e-9, 100., 'uniform'), # L2 regularization\n",
    "                    'reg_alpha': Real(1e-9, 100., 'uniform'), # L1 regularization\n",
    "                    'n_estimators': Integer(50, 500)}\n",
    "\n",
    "    bayesian = BayesSearchCV(\n",
    "        estimator=estimator,\n",
    "        search_spaces=search_spaces,\n",
    "        scoring = 'roc_auc',\n",
    "        n_iter = 25,\n",
    "        n_jobs = -1, # uses all available processors\n",
    "        cv = 5, # make (stratified) k-fold CV; using the StratifiedKFold method\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    bayesian.fit(X_train, y_train)\n",
    "\n",
    "    return bayesian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 (OPTIONAL) XGBoost Classifier – Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test, i):\n",
    "    # configure to select a subset of features\n",
    "    fs = SelectFromModel(XGBClassifier(n_estimators=i), max_features=10)\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    feature_names = fs.get_feature_names_out(input_features)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set at 10% FPR – normally used in related bibliography\n",
    "\n",
    "def getThresholdFromROC(y_true, y_pred, fpr_desired = .1):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred,)\n",
    "    return np.interp(fpr_desired, fpr, thresholds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the classification model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model,X_test,y_test):\n",
    "    \n",
    "    yhat_test_prob = model.predict_proba(X_test)\n",
    "\n",
    "    th = getThresholdFromROC(y_test,yhat_test_prob[:,1])\n",
    "    yhat_test = yhat_test_prob[:,1] > th\n",
    "\n",
    "    # ROC AUC\n",
    "    AUC =  roc_auc_score(y_test, yhat_test_prob[:,1])*100\n",
    "\n",
    "    # Confusion Matrix \n",
    "    CM = confusion_matrix(y_test,yhat_test)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "\n",
    "    # Print Classification scores\n",
    "    Sn = (TP/(TP+FN))*100\n",
    "    Sp = (TN/(TN+FP))*100\n",
    "    PPV = (TP/(TP+FP))*100\n",
    "    NPV = (TN/(TN+FN))*100\n",
    "\n",
    "    train_AUC = model.best_score_*100\n",
    "    #print('Grid search parameters: ', model.best_params_)\n",
    "\n",
    "    return train_AUC, AUC, Sn, Sp, PPV, NPV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch classification models\n",
    "\n",
    "1. For all defined feature sets\n",
    "2. Make 10 iterations (to average classification scores)\n",
    "3. Launch all scenarios defined in expDict. For each scenario:\n",
    "    \n",
    "    - Grid search (depth, n_estimators, learning rate) + 5-fold cross validation\n",
    "    - Bayesian optimization (preferred over Grid search)\n",
    "    - (OPTIONAL) Keep the 10 most informative features\n",
    "<br/><br/>\n",
    "    \n",
    "4. Print SHAP-values of the best performing model\n",
    "5. Stats of classification scores among nIts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIts = 10\n",
    "nSets = input_features.keys()\n",
    "nExps = expDict.keys()\n",
    "values = ['train_AUC', 'AUC', 'Sn', 'Sp', 'PPV', 'NPV','bestModel']\n",
    "\n",
    "# Create results dictionary\n",
    "results = {}\n",
    "for set in nSets:\n",
    "    results[set] = {}\n",
    "    for iter in range(nIts):\n",
    "        results[set][iter] = {}\n",
    "        for exp in nExps:\n",
    "            results[set][iter][exp] = {}\n",
    "            for val in values:\n",
    "                results[set][iter][exp][val] = None\n",
    "\n",
    "\n",
    "for set in nSets:\n",
    "\n",
    "    print(set)\n",
    "\n",
    "    X1 = dfImpact[input_features[set]]\n",
    "    y1 = dfImpact[Outcome]\n",
    "    #y1 = np.array(dfImpact[Outcome] == 1).astype(int) & np.array(dfImpact['cSection'] == 0).astype(int) \n",
    "    X2 = dfFedoc[input_features[set]]\n",
    "    y2 = dfFedoc[Outcome]\n",
    "    #y2 = np.array(dfFedoc[Outcome] == 1).astype(int) & np.array(dfFedoc['cSection'] == 0).astype(int)\n",
    "\n",
    "    for iter in tqdm(range(nIts)):\n",
    "\n",
    "        X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = .3, random_state=iter, stratify = y1)\n",
    "        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = .3, random_state=iter, stratify = y2)\n",
    "\n",
    "        for exp in nExps: \n",
    "\n",
    "            X_train = globals()[expDict[exp][0]].astype(float)\n",
    "            y_train = globals()[expDict[exp][1]].astype(float)\n",
    "            X_test = globals()[expDict[exp][2]].astype(float)\n",
    "            y_test = globals()[expDict[exp][3]].astype(float)\n",
    "\n",
    "            # In the test set, predict only in the group from 31 to 33 weeks (max. overlap for GA_US in both cohorts)\n",
    "            #to_include = X_test['GA_US'].between(31,33,inclusive=True)\n",
    "            #X_test = X_test[to_include]\n",
    "            #y_test = y_test[to_include]\n",
    "\n",
    "            # Grid search & 5-fold CV to find the best performing model, while preventing overfitting\n",
    "            #best_model = gridSearch_CV(X_train,y_train)\n",
    "            best_model = BayesSearch_CV(X_train,y_train)\n",
    "            # Model evaluation on test data\n",
    "            train_AUC, AUC, Sn, Sp, PPV, NPV = model_evaluation(best_model,X_test,y_test)\n",
    "\n",
    "            results[set][iter][exp]['train_AUC'] = np.round(train_AUC,1)\n",
    "            results[set][iter][exp]['AUC'] = np.round(AUC,1)\n",
    "            results[set][iter][exp]['Sn'] = np.round(Sn,1)\n",
    "            results[set][iter][exp]['Sp'] = np.round(Sp,1)\n",
    "            results[set][iter][exp]['PPV'] = np.round(PPV,1)\n",
    "            results[set][iter][exp]['NPV'] = np.round(NPV,1)\n",
    "            results[set][iter][exp]['bestModel'] = best_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ROC(model,X_test,y_test,set,exp):\n",
    "\n",
    "    yhat_test_prob = model.predict_proba(X_test)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test,  yhat_test_prob[:,1])\n",
    "\n",
    "    # Create ROC curve\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=fpr ,y=tpr, \n",
    "                            name = ('Classifier (AUC = %.2f)' % (round(roc_auc_score(y_test, yhat_test_prob[:,1])*100))),\n",
    "                            line=dict(color='royalblue', width=4, dash='solid')))\n",
    "    fig.add_trace(go.Scatter(x=[0,1],y=[0,1], \n",
    "                            name = 'Random Chance', \n",
    "                            line=dict(color='black', width=4, dash='dot')))\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"False Positive Rate\",\n",
    "        yaxis_title=\"True Positive Rate\", \n",
    "        height=600,\n",
    "        width=600, \n",
    "        legend=dict(yanchor=\"top\", y=0.98, xanchor=\"left\", x=0.02))\n",
    "\n",
    "    #fig.show()\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image('/Users/sergio/Desktop/' + set + exp + '_ROC.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_SHAP(model,X_test,set,exp):\n",
    "\n",
    "    # Fits the explainer\n",
    "    explainer = shap.Explainer(model.predict, X_test)\n",
    "    # Calculates the SHAP values - It takes some time\n",
    "    shap_values = explainer(X_test)\n",
    "    shap.plots.beeswarm(shap_values,show=False)\n",
    "    plt.savefig('/Users/sergio/Desktop/' + set + exp + '_SHAP.png',bbox_inches ='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results (best performing model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in nSets:\n",
    "\n",
    "    X1 = dfImpact[input_features[set]]\n",
    "    y1 = dfImpact[Outcome]\n",
    "    X2 = dfFedoc[input_features[set]]\n",
    "    y2 = dfFedoc[Outcome]\n",
    "\n",
    "    for exp in nExps:\n",
    "\n",
    "        listAUC = ([results[set][iter][exp]['AUC'] for iter in range(nIts)])\n",
    "        iter = listAUC.index(max(listAUC))\n",
    "\n",
    "        X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = .3, random_state=iter, stratify = y1)\n",
    "        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = .3, random_state=iter, stratify = y2)\n",
    "\n",
    "        model = results[set][iter][exp]['bestModel']\n",
    "        X_test = globals()[expDict[exp][2]].astype(float)\n",
    "        y_test = globals()[expDict[exp][3]].astype(float)\n",
    "\n",
    "        print_ROC(model,X_test,y_test,set,exp)\n",
    "        print_SHAP(model,X_test,set,exp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extended results are saved in json format. The model object is deleted before saving numeric accuracy results (AUC_train, AUC, Sn, Sp, PPV, NPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in nSets:\n",
    "    for iter in range(nIts):\n",
    "        for exp in nExps: \n",
    "            del results[set][iter][exp]['bestModel']\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/Users/sergio/Desktop/results.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean & STD across iterations for each set of features and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC_train')\n",
    "for exp in nExps: \n",
    "    for set in nSets:\n",
    "        AUC_train = ([results[set][iter][exp]['train_AUC'] for iter in range(nIts)])\n",
    "        print(str(np.around(np.mean(AUC_train),decimals=1)) + u\" \\u00B1 \" + str(np.around(np.std(AUC_train),decimals=1)))\n",
    "\n",
    "print('\\nAUC_test')\n",
    "for exp in nExps: \n",
    "    for set in nSets:\n",
    "        AUC = ([results[set][iter][exp]['AUC'] for iter in range(nIts)])\n",
    "        print(str(np.around(np.mean(AUC),decimals=1)) + u\" \\u00B1 \" + str(np.around(np.std(AUC),decimals=1)))\n",
    "\n",
    "print('\\nSensitivity')\n",
    "for exp in nExps: \n",
    "    for set in nSets:        \n",
    "        Sn = ([results[set][iter][exp]['Sn'] for iter in range(nIts)])\n",
    "        print(str(np.around(np.mean(Sn),decimals=1)) + u\" \\u00B1 \" + str(np.around(np.std(Sn),decimals=1)))\n",
    "\n",
    "print('\\nSpecificity')\n",
    "for exp in nExps: \n",
    "    for set in nSets:\n",
    "        Sp = ([results[set][iter][exp]['Sp'] for iter in range(nIts)])\n",
    "        print(str(np.around(np.mean(Sp),decimals=1)) + u\" \\u00B1 \" + str(np.around(np.std(Sp),decimals=1)))\n",
    "\n",
    "print('\\nPPV')\n",
    "for exp in nExps: \n",
    "    for set in nSets:\n",
    "        PPV = ([results[set][iter][exp]['PPV'] for iter in range(nIts)])\n",
    "        print(str(np.around(np.mean(PPV),decimals=1)) + u\" \\u00B1 \" + str(np.around(np.std(PPV),decimals=1)))\n",
    "\n",
    "print('\\nNPV')\n",
    "for exp in nExps: \n",
    "    for set in nSets:\n",
    "        NPV = ([results[set][iter][exp]['NPV'] for iter in range(nIts)])\n",
    "        print(str(np.around(np.mean(NPV),decimals=1)) + u\" \\u00B1 \" + str(np.around(np.std(NPV),decimals=1)))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
